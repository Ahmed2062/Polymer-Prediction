{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02e732fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T12:30:17.820749Z",
     "iopub.status.busy": "2025-06-29T12:30:17.820416Z",
     "iopub.status.idle": "2025-06-29T12:38:57.920181Z",
     "shell.execute_reply": "2025-06-29T12:38:57.918941Z"
    },
    "papermill": {
     "duration": 520.106214,
     "end_time": "2025-06-29T12:38:57.921980",
     "exception": false,
     "start_time": "2025-06-29T12:30:17.815766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 12:30:42.427445: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751200242.702063      13 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751200242.784728      13 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7973 entries, 0 to 7972\n",
      "Data columns (total 7 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   id       7973 non-null   int64  \n",
      " 1   SMILES   7973 non-null   object \n",
      " 2   Tg       511 non-null    float64\n",
      " 3   FFV      7030 non-null   float64\n",
      " 4   Tc       737 non-null    float64\n",
      " 5   Density  613 non-null    float64\n",
      " 6   Rg       614 non-null    float64\n",
      "dtypes: float64(5), int64(1), object(1)\n",
      "memory usage: 436.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Tg</th>\n",
       "      <th>FFV</th>\n",
       "      <th>Tc</th>\n",
       "      <th>Density</th>\n",
       "      <th>Rg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>*CC(*)c1ccccc1C(=O)OCCCCCC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.374645</td>\n",
       "      <td>0.205667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>*Nc1ccc([C@H](CCC)c2ccc(C3(c4ccc([C@@H](CCC)c5...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.370410</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>*Oc1ccc(S(=O)(=O)c2ccc(Oc3ccc(C4(c5ccc(Oc6ccc(...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.378860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>*Nc1ccc(-c2c(-c3ccc(C)cc3)c(-c3ccc(C)cc3)c(N*)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.387324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>*Oc1ccc(OC(=O)c2cc(OCCCCCCCCCOCC3CCCN3c3ccc([N...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.355470</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7968</th>\n",
       "      <td>*Oc1cc(CCCCCCCC)cc(OC(=O)c2cccc(C(*)=O)c2)c1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.367498</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7969</th>\n",
       "      <td>*C(=O)OCCN(CCOC(=O)c1ccc2c(c1)C(=O)N(c1cccc(N3...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.353280</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7970</th>\n",
       "      <td>*c1cc(C(=O)NCCCCCCCC)cc(N2C(=O)c3ccc(-c4ccc5c(...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.369411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7971</th>\n",
       "      <td>*C=C(*)c1ccccc1C</td>\n",
       "      <td>261.662355</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7972</th>\n",
       "      <td>*c1ccc(OCCCCCCCCCCCOC(=O)CCCCC(=O)OCCCCCCCCCCC...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.374049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7973 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text          Tg       FFV  \\\n",
       "0                            *CC(*)c1ccccc1C(=O)OCCCCCC         NaN  0.374645   \n",
       "1     *Nc1ccc([C@H](CCC)c2ccc(C3(c4ccc([C@@H](CCC)c5...         NaN  0.370410   \n",
       "2     *Oc1ccc(S(=O)(=O)c2ccc(Oc3ccc(C4(c5ccc(Oc6ccc(...         NaN  0.378860   \n",
       "3     *Nc1ccc(-c2c(-c3ccc(C)cc3)c(-c3ccc(C)cc3)c(N*)...         NaN  0.387324   \n",
       "4     *Oc1ccc(OC(=O)c2cc(OCCCCCCCCCOCC3CCCN3c3ccc([N...         NaN  0.355470   \n",
       "...                                                 ...         ...       ...   \n",
       "7968       *Oc1cc(CCCCCCCC)cc(OC(=O)c2cccc(C(*)=O)c2)c1         NaN  0.367498   \n",
       "7969  *C(=O)OCCN(CCOC(=O)c1ccc2c(c1)C(=O)N(c1cccc(N3...         NaN  0.353280   \n",
       "7970  *c1cc(C(=O)NCCCCCCCC)cc(N2C(=O)c3ccc(-c4ccc5c(...         NaN  0.369411   \n",
       "7971                                   *C=C(*)c1ccccc1C  261.662355       NaN   \n",
       "7972  *c1ccc(OCCCCCCCCCCCOC(=O)CCCCC(=O)OCCCCCCCCCCC...         NaN  0.374049   \n",
       "\n",
       "            Tc  Density  Rg  \n",
       "0     0.205667      NaN NaN  \n",
       "1          NaN      NaN NaN  \n",
       "2          NaN      NaN NaN  \n",
       "3          NaN      NaN NaN  \n",
       "4          NaN      NaN NaN  \n",
       "...        ...      ...  ..  \n",
       "7968       NaN      NaN NaN  \n",
       "7969       NaN      NaN NaN  \n",
       "7970       NaN      NaN NaN  \n",
       "7971       NaN      NaN NaN  \n",
       "7972       NaN      NaN NaN  \n",
       "\n",
       "[7973 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for target: Tg\n",
      "Extracting features for NN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 12:31:23.596190: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating for target: Tg\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "MSE: 3552.7252\n",
      "MAE: 45.4965\n",
      "RMSE: 59.6047\n",
      "Correlation: 0.8167\n",
      "\n",
      "Training for target: FFV\n",
      "Extracting features from training data...\n",
      "Evaluating for target: FFV\n",
      "MSE: 0.0003\n",
      "MAE: 0.0085\n",
      "RMSE: 0.0185\n",
      "Correlation: 0.8258\n",
      "\n",
      "Training for target: Tc\n",
      "Extracting features from training data...\n",
      "Evaluating for target: Tc\n",
      "MSE: 0.0025\n",
      "MAE: 0.0320\n",
      "RMSE: 0.0495\n",
      "Correlation: 0.8511\n",
      "\n",
      "Training for target: Density\n",
      "Extracting features for NN...\n",
      "Extracting features from training data...\n",
      "Evaluating for target: Density\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "MSE: 0.0144\n",
      "MAE: 0.0655\n",
      "RMSE: 0.1200\n",
      "Correlation: 0.4715\n",
      "\n",
      "Training for target: Rg\n",
      "Extracting features for NN...\n",
      "Evaluating for target: Rg\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "MSE: 6.6502\n",
      "MAE: 1.8487\n",
      "RMSE: 2.5788\n",
      "Correlation: 0.8340\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/2864580451.py:260: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[136.27418518 183.22776794 129.45050049]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  submit.iloc[:, 1:] = RESULT\n",
      "/tmp/ipykernel_13/2864580451.py:260: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.37532607 0.37305316 0.35220748]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  submit.iloc[:, 1:] = RESULT\n",
      "/tmp/ipykernel_13/2864580451.py:260: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.21702242 0.23187959 0.25791746]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  submit.iloc[:, 1:] = RESULT\n",
      "/tmp/ipykernel_13/2864580451.py:260: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1.1165309  1.10296977 1.12818265]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  submit.iloc[:, 1:] = RESULT\n",
      "/tmp/ipykernel_13/2864580451.py:260: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[20.66993713 22.36182976 19.00396538]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  submit.iloc[:, 1:] = RESULT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Tg</th>\n",
       "      <th>FFV</th>\n",
       "      <th>Tc</th>\n",
       "      <th>Density</th>\n",
       "      <th>Rg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1109053969</td>\n",
       "      <td>136.274185</td>\n",
       "      <td>0.375326</td>\n",
       "      <td>0.217022</td>\n",
       "      <td>1.116531</td>\n",
       "      <td>20.669937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1422188626</td>\n",
       "      <td>183.227768</td>\n",
       "      <td>0.373053</td>\n",
       "      <td>0.231880</td>\n",
       "      <td>1.102970</td>\n",
       "      <td>22.361830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2032016830</td>\n",
       "      <td>129.450500</td>\n",
       "      <td>0.352207</td>\n",
       "      <td>0.257917</td>\n",
       "      <td>1.128183</td>\n",
       "      <td>19.003965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id          Tg       FFV        Tc   Density         Rg\n",
       "0  1109053969  136.274185  0.375326  0.217022  1.116531  20.669937\n",
       "1  1422188626  183.227768  0.373053  0.231880  1.102970  22.361830\n",
       "2  2032016830  129.450500  0.352207  0.257917  1.128183  19.003965"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, RobertaTokenizer, RobertaModel\n",
    "transformers.logging.set_verbosity_error()\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# 1. Load the dataset\n",
    "data0 = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/train.csv')\n",
    "display(data0.info())\n",
    "targets = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "data = data0.drop('id',axis=1)\n",
    "data.columns=['text']+targets\n",
    "display(data)\n",
    "\n",
    "TEST0 = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/test.csv')\n",
    "TEST = TEST0[['SMILES']]\n",
    "TEST.columns=['text']\n",
    "\n",
    "# 2. Tokenization and data preparation\n",
    "model_path=\"/kaggle/input/c/transformers/default/1/ChemBERTa-77M-MLM\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "max_len = 128\n",
    "\n",
    "def create_data(text):\n",
    "    encoded = tokenizer.batch_encode_plus(\n",
    "        text.tolist(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "    input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
    "    attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"float64\")\n",
    "\n",
    "    return {\"input_ids\": input_ids,\"attention_mask\": attention_masks}\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def extract_roberta_features(texts, model_path, batch_size=32, max_len=512):\n",
    "    \"\"\"\n",
    "    Extract features from text using RoBERTa model\n",
    "\n",
    "    Args:\n",
    "        texts: List of input text strings\n",
    "        model_path: Path to model (local or Hugging Face hub name)\n",
    "        batch_size: Batch size for processing\n",
    "        max_len: Maximum sequence length\n",
    "\n",
    "    Returns:\n",
    "        numpy array of extracted features\n",
    "    \"\"\"\n",
    "    # Load model and tokenizer\n",
    "    model = AutoModel.from_pretrained(model_path, local_files_only=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    features = []\n",
    "\n",
    "    # Process in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "\n",
    "        # Tokenization (using PyTorch tensors)\n",
    "        encoded = tokenizer(\n",
    "            batch_texts,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Move tensors to the same device as model\n",
    "        encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "\n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded)\n",
    "\n",
    "        # Average pooling (mean of last hidden states)\n",
    "        pooled_features = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        # Move to CPU and convert to numpy\n",
    "        features.append(pooled_features.cpu().numpy())\n",
    "\n",
    "    return np.vstack(features)\n",
    "\n",
    "# Learning rate scheduler\n",
    "def scheduler(epoch):\n",
    "    learning_rate = 2e-5\n",
    "    if epoch == 0:\n",
    "        return learning_rate * 0.1\n",
    "    else:\n",
    "        return learning_rate * (0.95 ** epoch)\n",
    "\n",
    "# 4. Train the model (modified for feature extraction approach)\n",
    "def train_model(train_texts, train_targets):\n",
    "    print(\"Extracting features from training data...\")\n",
    "    train_features = extract_roberta_features(train_texts, model_path)\n",
    "\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        tree_method=\"hist\",\n",
    "        verbosity=0\n",
    "    )\n",
    "\n",
    "    model.fit(train_features, train_targets)\n",
    "    return model\n",
    "\n",
    "# 5. Predict and evaluate (for feature extraction approach)\n",
    "def evaluate_model(model, test_targets):\n",
    "    # Extract features from test data\n",
    "    print(\"Extracting features from test data...\")\n",
    "    test_texts = test['text'].tolist()\n",
    "    test_features = extract_roberta_features(test_texts, model_path)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(test_features)\n",
    "    predictions = predictions.flatten()\n",
    "\n",
    "    # Evaluation metrics\n",
    "    mse = np.mean((predictions - test_targets) ** 2)\n",
    "    mae = np.mean(np.abs(predictions - test_targets))\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    # Correlation coefficient\n",
    "    correlation = np.corrcoef(predictions, test_targets)[0, 1]\n",
    "\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"Correlation: {correlation:.4f}\")\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def build_regression_model(input_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=(input_dim,)),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                  loss='mse',\n",
    "                  metrics=['mae'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_nn_model(train_texts, train_targets):\n",
    "    print(\"Extracting features for NN...\")\n",
    "    train_features = extract_roberta_features(train_texts, model_path)\n",
    "\n",
    "    model = build_regression_model(train_features.shape[1])\n",
    "    model.fit(\n",
    "        train_features, train_targets,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nn_models = {}\n",
    "    xgb_models = {}\n",
    "\n",
    "    for target in targets:\n",
    "        filtered_df = data.dropna(subset=[target])\n",
    "        train, test = train_test_split(filtered_df, test_size=0.1, random_state=42)\n",
    "\n",
    "        train_texts = train['text'].tolist()\n",
    "        test_texts = test['text'].tolist()\n",
    "        train_targets = train[target].values\n",
    "        test_targets = test[target].values\n",
    "\n",
    "        print(f\"\\nTraining for target: {target}\")\n",
    "\n",
    "        if target in ['Tg', 'Rg']:\n",
    "            model = train_nn_model(train_texts, train_targets)\n",
    "            nn_models[target] = model\n",
    "        elif target in ['Tc', 'FFV']:\n",
    "            model = train_model(train_texts, train_targets)\n",
    "            xgb_models[target] = model\n",
    "        elif target == 'Density':\n",
    "            nn_model = train_nn_model(train_texts, train_targets)\n",
    "            xgb_model = train_model(train_texts, train_targets)\n",
    "            nn_models[target] = nn_model\n",
    "            xgb_models[target] = xgb_model\n",
    "\n",
    "        print(f\"Evaluating for target: {target}\")\n",
    "        test_features = extract_roberta_features(test_texts, model_path)\n",
    "\n",
    "        if target in ['Tg', 'Rg']:\n",
    "            predictions = nn_models[target].predict(test_features).flatten()\n",
    "        elif target in ['Tc', 'FFV']:\n",
    "            predictions = xgb_models[target].predict(test_features)\n",
    "        elif target == 'Density':\n",
    "            nn_pred = nn_models[target].predict(test_features).flatten()\n",
    "            xgb_pred = xgb_models[target].predict(test_features)\n",
    "            predictions = 0.2 * nn_pred + 0.8 * xgb_pred\n",
    "\n",
    "        mse = np.mean((predictions - test_targets) ** 2)\n",
    "        mae = np.mean(np.abs(predictions - test_targets))\n",
    "        rmse = np.sqrt(mse)\n",
    "        correlation = np.corrcoef(predictions, test_targets)[0, 1]\n",
    "\n",
    "        print(f\"MSE: {mse:.4f}\")\n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "        print(f\"Correlation: {correlation:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "test_texts = TEST['text'].tolist()\n",
    "test_features = extract_roberta_features(test_texts, model_path)\n",
    "\n",
    "RESULT = np.zeros((len(TEST), len(targets)))\n",
    "\n",
    "for i, target in enumerate(targets):\n",
    "    if target in ['Tg', 'Rg']:\n",
    "        pred = nn_models[target].predict(test_features).flatten()\n",
    "    elif target in ['Tc', 'FFV']:\n",
    "        pred = xgb_models[target].predict(test_features)\n",
    "    elif target == 'Density':\n",
    "        nn_pred = nn_models[target].predict(test_features).flatten()\n",
    "        xgb_pred = xgb_models[target].predict(test_features)\n",
    "        pred = 0.2 * nn_pred + 0.8 * xgb_pred\n",
    "\n",
    "    RESULT[:, i] = pred\n",
    "\n",
    "submit = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/sample_submission.csv')\n",
    "submit.iloc[:, 1:] = RESULT\n",
    "submit.to_csv('submission_chemberta.csv', index=False)\n",
    "display(submit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "469afc80",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-29T12:38:57.932931Z",
     "iopub.status.busy": "2025-06-29T12:38:57.932547Z",
     "iopub.status.idle": "2025-06-29T12:43:49.748859Z",
     "shell.execute_reply": "2025-06-29T12:43:49.747341Z"
    },
    "papermill": {
     "duration": 291.824438,
     "end_time": "2025-06-29T12:43:49.751049",
     "exception": false,
     "start_time": "2025-06-29T12:38:57.926611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  PyTorch: 2.6.0+cu124\n",
      "✅  PyG    : 2.6.1\n",
      "✅  RDKit  : 2023.09.4\n",
      "RDKit test (atoms): 3\n",
      "Using device: cpu\n",
      "\n",
      "Training for target: Tg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/3408641916.py:90: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  x = torch.tensor([atom_features(atom) for atom in mol.GetAtoms()], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.4727, MAE: 0.5474, RMSE: 0.6876, Correlation: 0.8133\n",
      "\n",
      "Training for target: FFV\n",
      "MSE: 0.2902, MAE: 0.3871, RMSE: 0.5387, Correlation: 0.9051\n",
      "\n",
      "Training for target: Tc\n",
      "MSE: 0.2704, MAE: 0.3615, RMSE: 0.5200, Correlation: 0.8729\n",
      "\n",
      "Training for target: Density\n",
      "MSE: 0.3268, MAE: 0.3121, RMSE: 0.5716, Correlation: 0.7722\n",
      "\n",
      "Training for target: Rg\n",
      "MSE: 0.5050, MAE: 0.4683, RMSE: 0.7107, Correlation: 0.7324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/3408641916.py:215: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[226.47952271 238.12884521 173.89105225]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  submission.iloc[:, 1:] = result\n",
      "/tmp/ipykernel_13/3408641916.py:215: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.37780359 0.38042289 0.3692404 ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  submission.iloc[:, 1:] = result\n",
      "/tmp/ipykernel_13/3408641916.py:215: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.24144965 0.26038876 0.30055571]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  submission.iloc[:, 1:] = result\n",
      "/tmp/ipykernel_13/3408641916.py:215: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1.16467452 1.05886388 1.0732559 ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  submission.iloc[:, 1:] = result\n",
      "/tmp/ipykernel_13/3408641916.py:215: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[21.97476006 19.50215149 20.248312  ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  submission.iloc[:, 1:] = result\n"
     ]
    }
   ],
   "source": [
    "import shutil, os, subprocess, sys\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 1. Fix the torch_spline_conv filename and move it to /kaggle/working\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "src = \"/kaggle/input/pyg-wheel/torch_spline_conv-1.2.2+pt26cpu-cp311-cp311-linux_x86_64 - Copy.whl\"\n",
    "dst = \"/kaggle/working/torch_spline_conv-1.2.2+pt26cpu-cp311-cp311-linux_x86_64.whl\"\n",
    "if not os.path.exists(dst):\n",
    "    shutil.copy(src, dst)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 2. Install the four other PyG wheels in one shot\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "!pip install --no-index --find-links /kaggle/input/pyg-wheel \\\n",
    "    torch_scatter torch_sparse torch_cluster torch_geometric -q\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 3. Install torch_spline_conv from the cleaned‑up filename\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "!pip install /kaggle/working/torch_spline_conv-1.2.2+pt26cpu-cp311-cp311-linux_x86_64.whl -q\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 4. Install RDKit directly from your wheel\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "!pip install /kaggle/input/rdkit-wheels/rdkit-2023.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl -q\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 5. Quick sanity check\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "try:\n",
    "    import torch, torch_geometric, rdkit\n",
    "    from rdkit import Chem\n",
    "    print(\"✅  PyTorch:\", torch.__version__)\n",
    "    print(\"✅  PyG    :\", torch_geometric.__version__)\n",
    "    print(\"✅  RDKit  :\", rdkit.__version__)\n",
    "    mol = Chem.MolFromSmiles(\"CCO\")\n",
    "    print(\"RDKit test (atoms):\", mol.GetNumAtoms())\n",
    "except Exception as e:\n",
    "    print(\"⚠️  Install check failed:\", e, file=sys.stderr)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "from rdkit import Chem\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "train_df = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/test.csv')\n",
    "train_df = train_df.rename(columns={\"SMILES\": \"text\"})\n",
    "test_df = test_df.rename(columns={\"SMILES\": \"text\"})\n",
    "targets = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "\n",
    "# Feature extractors\n",
    "def atom_features(atom):\n",
    "    return np.array([\n",
    "        atom.GetAtomicNum(),\n",
    "        atom.GetDegree(),\n",
    "        atom.GetFormalCharge(),\n",
    "        int(atom.GetHybridization()),\n",
    "        atom.GetIsAromatic(),\n",
    "        atom.GetTotalNumHs()\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "def bond_features(bond):\n",
    "    return np.array([\n",
    "        bond.GetBondTypeAsDouble(),\n",
    "        bond.GetIsConjugated(),\n",
    "        bond.IsInRing()\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "def smiles_to_graph(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    \n",
    "    Chem.Kekulize(mol, clearAromaticFlags=True)\n",
    "    mol = Chem.AddHs(mol)\n",
    "\n",
    "    x = torch.tensor([atom_features(atom) for atom in mol.GetAtoms()], dtype=torch.float)\n",
    "\n",
    "    edge_index = []\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        edge_index.append([i, j])\n",
    "        edge_index.append([j, i])\n",
    "\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    return Data(x=x, edge_index=edge_index)\n",
    "\n",
    "class PolymerDataset(Dataset):\n",
    "    def __init__(self, df, targets=None):\n",
    "        self.graphs = [smiles_to_graph(smile) for smile in df['text']]\n",
    "        self.graphs = [g for g in self.graphs if g is not None]\n",
    "        self.targets = torch.tensor(targets[:len(self.graphs)], dtype=torch.float32).view(-1, 1) if targets is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.targets is not None:\n",
    "            return self.graphs[idx], self.targets[idx]\n",
    "        return self.graphs[idx]\n",
    "\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim=128):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_dim, heads=4, concat=False)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.conv2 = GATConv(hidden_dim, hidden_dim, heads=4, concat=False)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.lin(x)\n",
    "\n",
    "def train_model(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data, target in loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data.x, data.edge_index, data.batch)\n",
    "            all_preds.append(output.cpu().numpy())\n",
    "            all_targets.append(target.cpu().numpy())\n",
    "    preds = np.concatenate(all_preds)\n",
    "    targets = np.concatenate(all_targets)\n",
    "    mse = np.mean((preds - targets) ** 2)\n",
    "    mae = np.mean(np.abs(preds - targets))\n",
    "    rmse = np.sqrt(mse)\n",
    "    corr = np.corrcoef(preds.flatten(), targets.flatten())[0, 1]\n",
    "    return mse, mae, rmse, corr, preds\n",
    "\n",
    "submission = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/sample_submission.csv')\n",
    "result = np.zeros((len(test_df), len(targets)))\n",
    "\n",
    "for i, target in enumerate(targets):\n",
    "    print(f\"\\nTraining for target: {target}\")\n",
    "    filtered = train_df.dropna(subset=[target])\n",
    "    train_data, val_data = train_test_split(filtered, test_size=0.1, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    y_train = scaler.fit_transform(train_data[[target]])\n",
    "    y_val = scaler.transform(val_data[[target]])\n",
    "\n",
    "    train_dataset = PolymerDataset(train_data, targets=y_train)\n",
    "    val_dataset = PolymerDataset(val_data, targets=y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "    model = GNNModel(in_channels=6).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(20):\n",
    "        train_loss = train_model(model, train_loader, optimizer, criterion)\n",
    "        scheduler.step()\n",
    "\n",
    "    mse, mae, rmse, corr, _ = evaluate_model(model, val_loader)\n",
    "    print(f\"MSE: {mse:.4f}, MAE: {mae:.4f}, RMSE: {rmse:.4f}, Correlation: {corr:.4f}\")\n",
    "\n",
    "    test_dataset = PolymerDataset(test_df)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data.x, data.edge_index, data.batch)\n",
    "            preds.append(output.cpu().numpy())\n",
    "    preds = np.concatenate(preds).flatten()\n",
    "    result[:, i] = scaler.inverse_transform(preds.reshape(-1, 1)).flatten()\n",
    "\n",
    "submission.iloc[:, 1:] = result\n",
    "submission.to_csv(\"submission_gnn.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "550e120e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T12:43:49.762481Z",
     "iopub.status.busy": "2025-06-29T12:43:49.762148Z",
     "iopub.status.idle": "2025-06-29T12:43:49.775816Z",
     "shell.execute_reply": "2025-06-29T12:43:49.774652Z"
    },
    "papermill": {
     "duration": 0.021382,
     "end_time": "2025-06-29T12:43:49.777540",
     "exception": false,
     "start_time": "2025-06-29T12:43:49.756158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load both submission files\n",
    "gnn_preds = pd.read_csv(\"submission_gnn.csv\")\n",
    "chemberta_preds = pd.read_csv(\"submission_chemberta.csv\")\n",
    "\n",
    "# Blend the predictions\n",
    "targets = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "ensemble = gnn_preds.copy()\n",
    "for target in targets:\n",
    "    if target in ['Tg', 'Rg']:\n",
    "        ensemble[target] = gnn_preds[target]  # GNN stronger\n",
    "    elif target in ['Tc', 'FFV']:\n",
    "        ensemble[target] = chemberta_preds[target]  # ChemBERTa stronger\n",
    "    elif target == 'Density':\n",
    "        ensemble[target] = gnn_preds[target]  # Assuming GNN is better\n",
    "\n",
    "# Save as the required submission file\n",
    "ensemble.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12609125,
     "sourceId": 74608,
     "sourceType": "competition"
    },
    {
     "datasetId": 7764926,
     "sourceId": 12318942,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7765007,
     "sourceId": 12319071,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 380893,
     "modelInstanceId": 359690,
     "sourceId": 442871,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 820.675041,
   "end_time": "2025-06-29T12:43:53.398844",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-29T12:30:12.723803",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
